{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"bz # Train machine learning models with confidence. Reproducible, efficient, and extensible. Why bz? # bz is a model training framework that eliminates the complexity of ML infrastructure so teams can focus on what matters most: building better models. We believe that every ML practitioner deserves powerful, reproducible training workflows without the overhead of custom infrastructure code. bz provides a unified command-line interface that works consistently across local development and production environments. Choose bz for its ability to reduce training boilerplate, ensure reproducibility, and enable seamless integrations with other tools in your ML pipeline. Use bz when training experiments, running hyperparameter searches, or deploying production models - it works seamlessly for both local and automated workflows. Quick Start # Add bz to your existing project: # Install bz pip install bz-cli # Set up bz for your project bz init # Define your model # Edit train.py to define your model, loss_fn, and optimizer # Start training bz train That's it! Your model is now training with automatic progress tracking, checkpointing, and metric logging. Common Workflows # Training and Evaluation # Train your models with confidence using bz's built-in training loop: # Basic training bz train # Training with custom configuration bz train --config my_config.json Hyperparameter Tuning # Optimize your model performance with automated hyperparameter search: # Optuna integration pip install bz-cli[optuna] # Add the optuna plugin to your bzconfig.json # Define parameters for your study # Run training with hyperparameter optimization bz train Custom Integrations # Extend bz with plugins for your specific needs: Experiment Tracking # # Weights & Biases integration pip install bz-cli[wandb] # Add the wandb plugin to your bzconfig.json # Train your models with advanced reporting bz train # TensorBoard logging pip install bz-cli[tensorboard] # Add the tensorboard plugin to your bzconfig.json # Train your models with TensorBoard logging bz train Custom Metrics and Plugins # Extend bz to include the metrics and integrations your team needs. # Create custom metrics from bz.metrics import Metric class CustomMetric(Metric): def compute(self, predictions, targets): return your_calculation(predictions, targets) # Build your own plugins from bz.plugins import Plugin class CustomPlugin(Plugin): def on_epoch_end(self, trainer): # Your custom logic here pass Now update bzconfig.json to use your new metric and plugin: { ... // bzconfig.json \"metrics\": [\"custom_metric\"], \"plugins\": [\"custom_plugin\"] }","title":"Home"},{"location":"#bz","text":"Train machine learning models with confidence. Reproducible, efficient, and extensible.","title":"bz"},{"location":"#why-bz","text":"bz is a model training framework that eliminates the complexity of ML infrastructure so teams can focus on what matters most: building better models. We believe that every ML practitioner deserves powerful, reproducible training workflows without the overhead of custom infrastructure code. bz provides a unified command-line interface that works consistently across local development and production environments. Choose bz for its ability to reduce training boilerplate, ensure reproducibility, and enable seamless integrations with other tools in your ML pipeline. Use bz when training experiments, running hyperparameter searches, or deploying production models - it works seamlessly for both local and automated workflows.","title":"Why bz?"},{"location":"#quick-start","text":"Add bz to your existing project: # Install bz pip install bz-cli # Set up bz for your project bz init # Define your model # Edit train.py to define your model, loss_fn, and optimizer # Start training bz train That's it! Your model is now training with automatic progress tracking, checkpointing, and metric logging.","title":"Quick Start"},{"location":"#common-workflows","text":"","title":"Common Workflows"},{"location":"#training-and-evaluation","text":"Train your models with confidence using bz's built-in training loop: # Basic training bz train # Training with custom configuration bz train --config my_config.json","title":"Training and Evaluation"},{"location":"#hyperparameter-tuning","text":"Optimize your model performance with automated hyperparameter search: # Optuna integration pip install bz-cli[optuna] # Add the optuna plugin to your bzconfig.json # Define parameters for your study # Run training with hyperparameter optimization bz train","title":"Hyperparameter Tuning"},{"location":"#custom-integrations","text":"Extend bz with plugins for your specific needs:","title":"Custom Integrations"},{"location":"#experiment-tracking","text":"# Weights & Biases integration pip install bz-cli[wandb] # Add the wandb plugin to your bzconfig.json # Train your models with advanced reporting bz train # TensorBoard logging pip install bz-cli[tensorboard] # Add the tensorboard plugin to your bzconfig.json # Train your models with TensorBoard logging bz train","title":"Experiment Tracking"},{"location":"#custom-metrics-and-plugins","text":"Extend bz to include the metrics and integrations your team needs. # Create custom metrics from bz.metrics import Metric class CustomMetric(Metric): def compute(self, predictions, targets): return your_calculation(predictions, targets) # Build your own plugins from bz.plugins import Plugin class CustomPlugin(Plugin): def on_epoch_end(self, trainer): # Your custom logic here pass Now update bzconfig.json to use your new metric and plugin: { ... // bzconfig.json \"metrics\": [\"custom_metric\"], \"plugins\": [\"custom_plugin\"] }","title":"Custom Metrics and Plugins"},{"location":"about/","text":"About # bz is a machine learning training framework that eliminates infrastructure complexity so teams can focus on building better models. Make ML training accessible, reproducible, and extensible for every practitioner. Get Involved # GitHub Repository Report Issues Contributing Guide","title":"About"},{"location":"about/#about","text":"bz is a machine learning training framework that eliminates infrastructure complexity so teams can focus on building better models. Make ML training accessible, reproducible, and extensible for every practitioner.","title":"About"},{"location":"about/#get-involved","text":"GitHub Repository Report Issues Contributing Guide","title":"Get Involved"},{"location":"developer-guide/","text":"Developer Guide # This guide is for contributors and developers working on the bz-cli project. Project Structure # bz-cli/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 bz/ # Core framework \u2502 \u2502 \u251c\u2500\u2500 __init__.py # Main trainer and core classes \u2502 \u2502 \u251c\u2500\u2500 cli.py # Command-line interface \u2502 \u2502 \u251c\u2500\u2500 metrics/ # Modular metrics system \u2502 \u2502 \u2514\u2500\u2500 plugins/ # Core plugin system \u2502 \u251c\u2500\u2500 bz_optuna/ # Optuna plugin package \u2502 \u251c\u2500\u2500 bz_wandb/ # WandB plugin package \u2502 \u251c\u2500\u2500 bz_tensorboard/ # TensorBoard plugin package \u2502 \u2514\u2500\u2500 bz_profiler/ # Profiler plugin package \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 examples/ # Example projects \u251c\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 pyproject.toml # Project configuration Development Setup # Prerequisites # Git Python 3.10 or higher uv Quick Start # # Clone and setup git clone https://github.com/your-org/bz-cli.git cd bz-cli # Install dependencies uv sync --all-extras --dev # Verify setup uv run pytest Development Commands # # Run tests uv run pytest # Linting and formatting uv run ruff check src tests uv run ruff check --fix src tests uv run black src tests # Type checking uv run mypy src tests Plugin Development # Creating a New Plugin # Create plugin structure bash uv init --plugin my-plugin Implement the plugin ```python # src/bz_myplugin/myplugin_plugin.py from typing import Optional, Dict, Any from bz.plugins import Plugin, PluginContext class MyPluginPlugin(Plugin): \"\"\"My custom plugin.\"\"\" name = \"myplugin\" def __init__( self, config: Optional[Dict[str, Any]] = None, **kwargs ) -> None: super().__init__(name=self.name, config=config, **kwargs) def start_training_session(self, context: PluginContext) -> None: \"\"\"Initialize plugin at training start.\"\"\" self.logger.info(\"MyPlugin initialized\") ``` Register the plugin ```python # src/bz_myplugin/ init .py from .myplugin_plugin import MyPluginPlugin all = [\"MyPluginPlugin\"] ``` Add entry point toml # pyproject.toml [project.entry-points.\"bz.plugins\"] myplugin = \"bz_myplugin.myplugin_plugin:MyPluginPlugin\" Plugin Testing # # tests/bz_myplugin/test_myplugin.py import pytest from unittest.mock import Mock, patch from bz_myplugin import MyPluginPlugin from bz.plugins import PluginContext def test_myplugin_initialization(): \"\"\"Test plugin initialization.\"\"\" plugin = MyPluginPlugin() assert plugin.name == \"myplugin\" def test_myplugin_start_training_session(): \"\"\"Test plugin start_training_session hook.\"\"\" plugin = MyPluginPlugin() context = Mock(spec=PluginContext) with patch.object(plugin.logger, 'info') as mock_info: plugin.start_training_session(context) mock_info.assert_called_once_with(\"MyPlugin initialized\") Contributing # Workflow # Fork and branch bash git checkout -b feature/my-feature Make changes and test bash uv run pytest uv run black src tests uv run ruff check src tests uv run mypy src tests Commit and push bash git add . git commit -m \"feat: add my feature\" git push origin feature/my-feature Create pull request Create a pull request to the main branch and a maintainer will review your changes. Getting Help # Check existing issues Search documentation Create issue with: Python version Error message Steps to reproduce","title":"Developer Guide"},{"location":"developer-guide/#developer-guide","text":"This guide is for contributors and developers working on the bz-cli project.","title":"Developer Guide"},{"location":"developer-guide/#project-structure","text":"bz-cli/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 bz/ # Core framework \u2502 \u2502 \u251c\u2500\u2500 __init__.py # Main trainer and core classes \u2502 \u2502 \u251c\u2500\u2500 cli.py # Command-line interface \u2502 \u2502 \u251c\u2500\u2500 metrics/ # Modular metrics system \u2502 \u2502 \u2514\u2500\u2500 plugins/ # Core plugin system \u2502 \u251c\u2500\u2500 bz_optuna/ # Optuna plugin package \u2502 \u251c\u2500\u2500 bz_wandb/ # WandB plugin package \u2502 \u251c\u2500\u2500 bz_tensorboard/ # TensorBoard plugin package \u2502 \u2514\u2500\u2500 bz_profiler/ # Profiler plugin package \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 examples/ # Example projects \u251c\u2500\u2500 docs/ # Documentation \u2514\u2500\u2500 pyproject.toml # Project configuration","title":"Project Structure"},{"location":"developer-guide/#development-setup","text":"","title":"Development Setup"},{"location":"developer-guide/#prerequisites","text":"Git Python 3.10 or higher uv","title":"Prerequisites"},{"location":"developer-guide/#quick-start","text":"# Clone and setup git clone https://github.com/your-org/bz-cli.git cd bz-cli # Install dependencies uv sync --all-extras --dev # Verify setup uv run pytest","title":"Quick Start"},{"location":"developer-guide/#development-commands","text":"# Run tests uv run pytest # Linting and formatting uv run ruff check src tests uv run ruff check --fix src tests uv run black src tests # Type checking uv run mypy src tests","title":"Development Commands"},{"location":"developer-guide/#plugin-development","text":"","title":"Plugin Development"},{"location":"developer-guide/#creating-a-new-plugin","text":"Create plugin structure bash uv init --plugin my-plugin Implement the plugin ```python # src/bz_myplugin/myplugin_plugin.py from typing import Optional, Dict, Any from bz.plugins import Plugin, PluginContext class MyPluginPlugin(Plugin): \"\"\"My custom plugin.\"\"\" name = \"myplugin\" def __init__( self, config: Optional[Dict[str, Any]] = None, **kwargs ) -> None: super().__init__(name=self.name, config=config, **kwargs) def start_training_session(self, context: PluginContext) -> None: \"\"\"Initialize plugin at training start.\"\"\" self.logger.info(\"MyPlugin initialized\") ``` Register the plugin ```python # src/bz_myplugin/ init .py from .myplugin_plugin import MyPluginPlugin all = [\"MyPluginPlugin\"] ``` Add entry point toml # pyproject.toml [project.entry-points.\"bz.plugins\"] myplugin = \"bz_myplugin.myplugin_plugin:MyPluginPlugin\"","title":"Creating a New Plugin"},{"location":"developer-guide/#plugin-testing","text":"# tests/bz_myplugin/test_myplugin.py import pytest from unittest.mock import Mock, patch from bz_myplugin import MyPluginPlugin from bz.plugins import PluginContext def test_myplugin_initialization(): \"\"\"Test plugin initialization.\"\"\" plugin = MyPluginPlugin() assert plugin.name == \"myplugin\" def test_myplugin_start_training_session(): \"\"\"Test plugin start_training_session hook.\"\"\" plugin = MyPluginPlugin() context = Mock(spec=PluginContext) with patch.object(plugin.logger, 'info') as mock_info: plugin.start_training_session(context) mock_info.assert_called_once_with(\"MyPlugin initialized\")","title":"Plugin Testing"},{"location":"developer-guide/#contributing","text":"","title":"Contributing"},{"location":"developer-guide/#workflow","text":"Fork and branch bash git checkout -b feature/my-feature Make changes and test bash uv run pytest uv run black src tests uv run ruff check src tests uv run mypy src tests Commit and push bash git add . git commit -m \"feat: add my feature\" git push origin feature/my-feature Create pull request Create a pull request to the main branch and a maintainer will review your changes.","title":"Workflow"},{"location":"developer-guide/#getting-help","text":"Check existing issues Search documentation Create issue with: Python version Error message Steps to reproduce","title":"Getting Help"},{"location":"getting-started/","text":"Quick Start Guide # Get your first model training with bz-cli in under 10 minutes. Installation # pip install bz-cli Create Your First Project # bz init my-first-model cd my-first-model This creates: - train.py - Your training script - model.py - Model definition - bzconfig.json - Configuration file - README.md - Project documentation Train Your Model # bz train You should see progress bars, metrics being logged, and checkpoints being saved. That's it! You've successfully trained your first model with bz-cli. What Just Happened? # bz-cli automatically handled: - Training loops and optimization - Metrics tracking (loss, accuracy) - Model checkpointing - Error handling and recovery Next Steps # Customize Your Model : Edit model.py to change the network architecture Add Experiment Tracking : Install the WandB plugin to track your experiments Optimize Hyperparameters : Use the Optuna plugin to find the best model configuration Troubleshooting # Installation Issues # Problem : pip install bz-cli fails Solution : Try upgrading pip first: python -m pip install --upgrade pip pip install bz-cli PyTorch Issues # Problem : PyTorch not found Solution : Install PyTorch: pip install torch torchvision Permission Issues # Problem : Permission denied during installation Solution : Use user installation: pip install --user bz-cli Need Help? # Documentation : Check the full documentation Examples : Browse working examples Issues : Report bugs on GitHub Issues Pro Tip : The bz --help command shows all available options and commands.","title":"Getting Started"},{"location":"getting-started/#quick-start-guide","text":"Get your first model training with bz-cli in under 10 minutes.","title":"Quick Start Guide"},{"location":"getting-started/#installation","text":"pip install bz-cli","title":"Installation"},{"location":"getting-started/#create-your-first-project","text":"bz init my-first-model cd my-first-model This creates: - train.py - Your training script - model.py - Model definition - bzconfig.json - Configuration file - README.md - Project documentation","title":"Create Your First Project"},{"location":"getting-started/#train-your-model","text":"bz train You should see progress bars, metrics being logged, and checkpoints being saved. That's it! You've successfully trained your first model with bz-cli.","title":"Train Your Model"},{"location":"getting-started/#what-just-happened","text":"bz-cli automatically handled: - Training loops and optimization - Metrics tracking (loss, accuracy) - Model checkpointing - Error handling and recovery","title":"What Just Happened?"},{"location":"getting-started/#next-steps","text":"Customize Your Model : Edit model.py to change the network architecture Add Experiment Tracking : Install the WandB plugin to track your experiments Optimize Hyperparameters : Use the Optuna plugin to find the best model configuration","title":"Next Steps"},{"location":"getting-started/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/#installation-issues","text":"Problem : pip install bz-cli fails Solution : Try upgrading pip first: python -m pip install --upgrade pip pip install bz-cli","title":"Installation Issues"},{"location":"getting-started/#pytorch-issues","text":"Problem : PyTorch not found Solution : Install PyTorch: pip install torch torchvision","title":"PyTorch Issues"},{"location":"getting-started/#permission-issues","text":"Problem : Permission denied during installation Solution : Use user installation: pip install --user bz-cli","title":"Permission Issues"},{"location":"getting-started/#need-help","text":"Documentation : Check the full documentation Examples : Browse working examples Issues : Report bugs on GitHub Issues Pro Tip : The bz --help command shows all available options and commands.","title":"Need Help?"},{"location":"user-guide/","text":"User Guide # This guide covers the essential aspects of using and extending the bz CLI tool. CLI Commands # The bz CLI provides a simple interface for training machine learning models. bz init # Initialize a new project with the required structure. bz init This creates: * train.py - Your training script * bzconfig.json - Configuration file bz train # Train your model using the current configuration. # Basic training bz train # Train with custom config bz train --config my_config.json # Train for specific epochs bz train --epochs 100 # Use specific device bz train --device cuda Options: - --config : Path to configuration file (default: bzconfig.json ) - --epochs : Number of training epochs (overrides config) - --device : Device to use ( cpu , cuda , or auto ) Configuration Schema # The bzconfig.json file controls all aspects of training. Here's the complete schema: { \"training\": { \"epochs\": 100, \"batch_size\": 32, \"learning_rate\": 0.001, \"device\": \"auto\", \"compile\": true, \"checkpoint_interval\": 10, \"validation_split\": 0.2 }, \"plugins\": [ \"console_out\", { \"tensorboard\": { \"log_dir\": \"runs/experiment\", \"flush_interval\": 10 } }, { \"early_stopping\": { \"patience\": 10, \"monitor\": \"validation_loss\", \"min_delta\": 0.001 } } ], \"metrics\": [ \"accuracy\", \"precision\", \"recall\", \"f1_score\" ] } Training Configuration # Field Type Default Description epochs int 1 Number of training epochs batch_size int 32 Training batch size learning_rate float 0.001 Learning rate device string \"auto\" Device selection compile bool true Enable model compilation checkpoint_interval int 0 Save checkpoint every N epochs validation_split float 0.2 Validation data fraction Plugin Configuration # Plugins can be specified as simple strings (using defaults) or as objects with custom configuration: { \"plugins\": [ \"console_out\", // Simple string \"early_stopping\", // Simple string { // Object with config \"tensorboard\": { \"log_dir\": \"custom/path\" } } ] } Available Plugins # console_out : Progress bars and training output early_stopping : Automatic training termination tensorboard : TensorBoard logging wandb : Weights & Biases integration profiler : Performance monitoring Training Script Structure # The train.py file is where you define your model, data, and training logic. Required Components # import torch import torch.nn as nn from torch.optim import Adam from torch.utils.data import DataLoader # 1. Model definition model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10) ) # 2. Loss function loss_fn = nn.CrossEntropyLoss() # 3. Optimizer optimizer = Adam(model.parameters(), lr=0.001) # 4. Data loaders training_loader = DataLoader(training_dataset, batch_size=32, shuffle=True) validation_loader = DataLoader(validation_dataset, batch_size=32) How It Works # Discovery : bz automatically discovers your train.py file Import : The file is imported and variables are extracted Validation : Required variables are checked for existence Training : The training loop is executed with your components Variable Requirements # Variable Required Type Description model Yes nn.Module Your PyTorch model loss_fn Yes Callable Loss function optimizer Yes Optimizer PyTorch optimizer training_loader Yes DataLoader Training data validation_loader No DataLoader Validation data metrics No List Custom metrics to track Example Complete Script # import torch import torch.nn as nn from torch.optim import Adam from torch.utils.data import DataLoader, random_split from torchvision import datasets, transforms # Data preparation transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) train_size = int(0.8 * len(dataset)) val_size = len(dataset) - train_size train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) # Data loaders training_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) validation_loader = DataLoader(val_dataset, batch_size=32) # Model model = nn.Sequential( nn.Flatten(), nn.Linear(784, 128), nn.ReLU(), nn.Dropout(0.2), nn.Linear(128, 10) ) # Training components loss_fn = nn.CrossEntropyLoss() optimizer = Adam(model.parameters(), lr=0.001) Custom Plugins # Plugins allow you to hook into the training lifecycle and add custom functionality. Plugin Base Class # from bz.plugins import Plugin class MyCustomPlugin(Plugin): def __init__(self, config=None): super().__init__() self.config = config or {} def start_training_session(self, context): \"\"\"Called when training begins\"\"\" print(f\"Starting training for {context.epochs} epochs\") def end_epoch(self, context): \"\"\"Called at the end of each epoch\"\"\" print(f\"Epoch {context.epoch}: Loss = {context.training_loss:.4f}\") def end_training_session(self, context): \"\"\"Called when training completes\"\"\" print(\"Training completed successfully!\") Available Hooks # Hook Description Context Available start_training_session Training begins Epochs, device, config start_epoch Epoch begins Current epoch number end_epoch Epoch ends Metrics, loss, epoch info end_training_session Training ends Final summary save_checkpoint Checkpoint saved Checkpoint path load_checkpoint Checkpoint loaded Checkpoint info Plugin Configuration # # In your train.py from my_plugins import MyCustomPlugin # Create plugin with config my_plugin = MyCustomPlugin({ \"log_file\": \"training.log\", \"verbose\": True }) # Add to plugins list plugins = [my_plugin] Plugin Discovery # Place your plugin files in a plugins/ directory or install them as Python packages. The framework automatically discovers plugins in the plugins list. Custom Metrics # Create custom metrics to track domain-specific performance measures. Metric Base Class # from bz.metrics import Metric import torch class CustomAccuracy(Metric): def __init__(self, threshold=0.5, name=None): super().__init__(name or \"custom_accuracy\") self.threshold = threshold self.correct = 0 self.total = 0 def reset(self): \"\"\"Reset metric state between epochs\"\"\" self.correct = 0 self.total = 0 def update(self, predictions, targets): \"\"\"Update metric with batch results\"\"\" # Apply threshold for binary classification pred_labels = (predictions > self.threshold).long() self.correct += (pred_labels == targets).sum().item() self.total += targets.size(0) def compute(self): \"\"\"Compute final metric value\"\"\" return self.correct / self.total if self.total > 0 else 0.0 Using Custom Metrics # # In your train.py from my_metrics import CustomAccuracy # Create metrics list metrics = [ CustomAccuracy(threshold=0.5), CustomAccuracy(threshold=0.7, name=\"strict_accuracy\") ] # The framework automatically tracks these during training Metric Requirements # Inherit from Metric : Use the base class for consistency Implement required methods : reset() , update() , compute() Handle edge cases : Check for division by zero, empty tensors Use descriptive names : Names appear in logs and TensorBoard Advanced Metrics # For complex metrics that require multiple passes or external data: class AdvancedMetric(Metric): def __init__(self): super().__init__(\"advanced_metric\") self.predictions = [] self.targets = [] def update(self, predictions, targets): \"\"\"Store predictions and targets for later computation\"\"\" self.predictions.extend(predictions.cpu().numpy()) self.targets.extend(targets.cpu().numpy()) def compute(self): \"\"\"Compute metric using all collected data\"\"\" # Complex computation here return complex_calculation(self.predictions, self.targets) Metric Registration # Custom metrics are automatically available when imported. No additional registration is required - just include them in your metrics list.","title":"User Guide"},{"location":"user-guide/#user-guide","text":"This guide covers the essential aspects of using and extending the bz CLI tool.","title":"User Guide"},{"location":"user-guide/#cli-commands","text":"The bz CLI provides a simple interface for training machine learning models.","title":"CLI Commands"},{"location":"user-guide/#bz-init","text":"Initialize a new project with the required structure. bz init This creates: * train.py - Your training script * bzconfig.json - Configuration file","title":"bz init"},{"location":"user-guide/#bz-train","text":"Train your model using the current configuration. # Basic training bz train # Train with custom config bz train --config my_config.json # Train for specific epochs bz train --epochs 100 # Use specific device bz train --device cuda Options: - --config : Path to configuration file (default: bzconfig.json ) - --epochs : Number of training epochs (overrides config) - --device : Device to use ( cpu , cuda , or auto )","title":"bz train"},{"location":"user-guide/#configuration-schema","text":"The bzconfig.json file controls all aspects of training. Here's the complete schema: { \"training\": { \"epochs\": 100, \"batch_size\": 32, \"learning_rate\": 0.001, \"device\": \"auto\", \"compile\": true, \"checkpoint_interval\": 10, \"validation_split\": 0.2 }, \"plugins\": [ \"console_out\", { \"tensorboard\": { \"log_dir\": \"runs/experiment\", \"flush_interval\": 10 } }, { \"early_stopping\": { \"patience\": 10, \"monitor\": \"validation_loss\", \"min_delta\": 0.001 } } ], \"metrics\": [ \"accuracy\", \"precision\", \"recall\", \"f1_score\" ] }","title":"Configuration Schema"},{"location":"user-guide/#training-configuration","text":"Field Type Default Description epochs int 1 Number of training epochs batch_size int 32 Training batch size learning_rate float 0.001 Learning rate device string \"auto\" Device selection compile bool true Enable model compilation checkpoint_interval int 0 Save checkpoint every N epochs validation_split float 0.2 Validation data fraction","title":"Training Configuration"},{"location":"user-guide/#plugin-configuration","text":"Plugins can be specified as simple strings (using defaults) or as objects with custom configuration: { \"plugins\": [ \"console_out\", // Simple string \"early_stopping\", // Simple string { // Object with config \"tensorboard\": { \"log_dir\": \"custom/path\" } } ] }","title":"Plugin Configuration"},{"location":"user-guide/#available-plugins","text":"console_out : Progress bars and training output early_stopping : Automatic training termination tensorboard : TensorBoard logging wandb : Weights & Biases integration profiler : Performance monitoring","title":"Available Plugins"},{"location":"user-guide/#training-script-structure","text":"The train.py file is where you define your model, data, and training logic.","title":"Training Script Structure"},{"location":"user-guide/#required-components","text":"import torch import torch.nn as nn from torch.optim import Adam from torch.utils.data import DataLoader # 1. Model definition model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10) ) # 2. Loss function loss_fn = nn.CrossEntropyLoss() # 3. Optimizer optimizer = Adam(model.parameters(), lr=0.001) # 4. Data loaders training_loader = DataLoader(training_dataset, batch_size=32, shuffle=True) validation_loader = DataLoader(validation_dataset, batch_size=32)","title":"Required Components"},{"location":"user-guide/#how-it-works","text":"Discovery : bz automatically discovers your train.py file Import : The file is imported and variables are extracted Validation : Required variables are checked for existence Training : The training loop is executed with your components","title":"How It Works"},{"location":"user-guide/#variable-requirements","text":"Variable Required Type Description model Yes nn.Module Your PyTorch model loss_fn Yes Callable Loss function optimizer Yes Optimizer PyTorch optimizer training_loader Yes DataLoader Training data validation_loader No DataLoader Validation data metrics No List Custom metrics to track","title":"Variable Requirements"},{"location":"user-guide/#example-complete-script","text":"import torch import torch.nn as nn from torch.optim import Adam from torch.utils.data import DataLoader, random_split from torchvision import datasets, transforms # Data preparation transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) train_size = int(0.8 * len(dataset)) val_size = len(dataset) - train_size train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) # Data loaders training_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) validation_loader = DataLoader(val_dataset, batch_size=32) # Model model = nn.Sequential( nn.Flatten(), nn.Linear(784, 128), nn.ReLU(), nn.Dropout(0.2), nn.Linear(128, 10) ) # Training components loss_fn = nn.CrossEntropyLoss() optimizer = Adam(model.parameters(), lr=0.001)","title":"Example Complete Script"},{"location":"user-guide/#custom-plugins","text":"Plugins allow you to hook into the training lifecycle and add custom functionality.","title":"Custom Plugins"},{"location":"user-guide/#plugin-base-class","text":"from bz.plugins import Plugin class MyCustomPlugin(Plugin): def __init__(self, config=None): super().__init__() self.config = config or {} def start_training_session(self, context): \"\"\"Called when training begins\"\"\" print(f\"Starting training for {context.epochs} epochs\") def end_epoch(self, context): \"\"\"Called at the end of each epoch\"\"\" print(f\"Epoch {context.epoch}: Loss = {context.training_loss:.4f}\") def end_training_session(self, context): \"\"\"Called when training completes\"\"\" print(\"Training completed successfully!\")","title":"Plugin Base Class"},{"location":"user-guide/#available-hooks","text":"Hook Description Context Available start_training_session Training begins Epochs, device, config start_epoch Epoch begins Current epoch number end_epoch Epoch ends Metrics, loss, epoch info end_training_session Training ends Final summary save_checkpoint Checkpoint saved Checkpoint path load_checkpoint Checkpoint loaded Checkpoint info","title":"Available Hooks"},{"location":"user-guide/#plugin-configuration_1","text":"# In your train.py from my_plugins import MyCustomPlugin # Create plugin with config my_plugin = MyCustomPlugin({ \"log_file\": \"training.log\", \"verbose\": True }) # Add to plugins list plugins = [my_plugin]","title":"Plugin Configuration"},{"location":"user-guide/#plugin-discovery","text":"Place your plugin files in a plugins/ directory or install them as Python packages. The framework automatically discovers plugins in the plugins list.","title":"Plugin Discovery"},{"location":"user-guide/#custom-metrics","text":"Create custom metrics to track domain-specific performance measures.","title":"Custom Metrics"},{"location":"user-guide/#metric-base-class","text":"from bz.metrics import Metric import torch class CustomAccuracy(Metric): def __init__(self, threshold=0.5, name=None): super().__init__(name or \"custom_accuracy\") self.threshold = threshold self.correct = 0 self.total = 0 def reset(self): \"\"\"Reset metric state between epochs\"\"\" self.correct = 0 self.total = 0 def update(self, predictions, targets): \"\"\"Update metric with batch results\"\"\" # Apply threshold for binary classification pred_labels = (predictions > self.threshold).long() self.correct += (pred_labels == targets).sum().item() self.total += targets.size(0) def compute(self): \"\"\"Compute final metric value\"\"\" return self.correct / self.total if self.total > 0 else 0.0","title":"Metric Base Class"},{"location":"user-guide/#using-custom-metrics","text":"# In your train.py from my_metrics import CustomAccuracy # Create metrics list metrics = [ CustomAccuracy(threshold=0.5), CustomAccuracy(threshold=0.7, name=\"strict_accuracy\") ] # The framework automatically tracks these during training","title":"Using Custom Metrics"},{"location":"user-guide/#metric-requirements","text":"Inherit from Metric : Use the base class for consistency Implement required methods : reset() , update() , compute() Handle edge cases : Check for division by zero, empty tensors Use descriptive names : Names appear in logs and TensorBoard","title":"Metric Requirements"},{"location":"user-guide/#advanced-metrics","text":"For complex metrics that require multiple passes or external data: class AdvancedMetric(Metric): def __init__(self): super().__init__(\"advanced_metric\") self.predictions = [] self.targets = [] def update(self, predictions, targets): \"\"\"Store predictions and targets for later computation\"\"\" self.predictions.extend(predictions.cpu().numpy()) self.targets.extend(targets.cpu().numpy()) def compute(self): \"\"\"Compute metric using all collected data\"\"\" # Complex computation here return complex_calculation(self.predictions, self.targets)","title":"Advanced Metrics"},{"location":"user-guide/#metric-registration","text":"Custom metrics are automatically available when imported. No additional registration is required - just include them in your metrics list.","title":"Metric Registration"}]}